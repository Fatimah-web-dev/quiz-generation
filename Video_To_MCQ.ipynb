{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatimah-web-dev/quiz-generation/blob/main/Video_To_MCQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1OMTyEUR2yC",
        "outputId": "f87bf1c5-87ce-482c-f490-9e4f35a5d44c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-10 12:20:00--  https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T122000Z&X-Amz-Expires=300&X-Amz-Signature=f3c91844912f619ff6267588e38725440c58bed1bb5296fd33d5bafaf6e8dfdd&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-10 12:20:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T122000Z&X-Amz-Expires=300&X-Amz-Signature=f3c91844912f619ff6267588e38725440c58bed1bb5296fd33d5bafaf6e8dfdd&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600444501 (573M) [application/octet-stream]\n",
            "Saving to: ‘s2v_reddit_2015_md.tar.gz.1’\n",
            "\n",
            "s2v_reddit_2015_md. 100%[===================>] 572.63M  74.1MB/s    in 7.9s    \n",
            "\n",
            "2025-03-10 12:20:08 (72.3 MB/s) - ‘s2v_reddit_2015_md.tar.gz.1’ saved [600444501/600444501]\n",
            "\n",
            "./._s2v_old\n",
            "./s2v_old/\n",
            "./s2v_old/._freqs.json\n",
            "./s2v_old/freqs.json\n",
            "./s2v_old/._vectors\n",
            "./s2v_old/vectors\n",
            "./s2v_old/._cfg\n",
            "./s2v_old/cfg\n",
            "./s2v_old/._strings.json\n",
            "./s2v_old/strings.json\n",
            "./s2v_old/._key2row\n",
            "./s2v_old/key2row\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t2TEh4CeRA8",
        "outputId": "f454bfdd-e917-4b8f-ce78-cde5f932aced",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXW5j2x0SlPd",
        "outputId": "f874b101-5c5c-4d60-ec3f-ccb4e588646e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install ffmpeg\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNM2Zq0mXgSH"
      },
      "source": [
        "### **1. Extract Audio**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPzBZzPGUyav",
        "outputId": "5a3f6105-0fb0-49af-8bb6-21fa59f05302"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "command = 'ffmpeg -i video2.mp4 -ab 160k -ar 44100 -vn audio1.wav'\n",
        "subprocess.call(command, shell=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwcKX540X9Oy"
      },
      "source": [
        "**2. Setup STT Service**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d8VyjBvzW6i3"
      },
      "outputs": [],
      "source": [
        "api_key = '30ee6b6a7f8a472eb0e1d086cf354078'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gVXqxA3_ikxN"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "\n",
        "upload_endpoint = 'https://api.assemblyai.com/v2/upload'\n",
        "transcript_endpoint = 'https://api.assemblyai.com/v2/transcript'\n",
        "\n",
        "headers_auth_only = {'authorization': api_key}\n",
        "\n",
        "headers = {\n",
        "    \"authorization\": api_key,\n",
        "    \"content-type\": \"application/json\"\n",
        "}\n",
        "\n",
        "CHUNK_SIZE = 6000000\n",
        "\n",
        "\n",
        "def upload(filename):\n",
        "    def read_file(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(CHUNK_SIZE)\n",
        "                if not data:\n",
        "                    break\n",
        "                yield data\n",
        "\n",
        "    upload_response = requests.post(upload_endpoint, headers=headers_auth_only, data=read_file(filename))\n",
        "    return upload_response.json()['upload_url']\n",
        "\n",
        "\n",
        "def transcribe(audio_url):\n",
        "    transcript_request = {\n",
        "        'audio_url': audio_url\n",
        "    }\n",
        "\n",
        "    transcript_response = requests.post(transcript_endpoint, json=transcript_request, headers=headers)\n",
        "    return transcript_response.json()['id']\n",
        "\n",
        "\n",
        "def poll(transcript_id):\n",
        "    polling_endpoint = transcript_endpoint + '/' + transcript_id\n",
        "    polling_response = requests.get(polling_endpoint, headers=headers)\n",
        "    return polling_response.json()\n",
        "\n",
        "\n",
        "def get_transcription_result_url(url):\n",
        "    transcribe_id = transcribe(url)\n",
        "    while True:\n",
        "        data = poll(transcribe_id)\n",
        "        if data['status'] == 'completed':\n",
        "            return data, None\n",
        "        elif data['status'] == 'error':\n",
        "            return data, data['error']\n",
        "\n",
        "        print(\"waiting for 30 seconds\")\n",
        "        time.sleep(30)\n",
        "\n",
        "\n",
        "def save_transcript(url, title):\n",
        "    data, error = get_transcription_result_url(url)\n",
        "\n",
        "    if data:\n",
        "        filename = title + '.txt'\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(data['text'])\n",
        "        print('Transcript saved')\n",
        "    elif error:\n",
        "        print(\"Error!!!\", error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr1CxvGRpXsH",
        "outputId": "7823928b-46c6-4612-d398-66bb47b5f6a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.2.19)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=qYNweeDHiyU\n",
            "[youtube] qYNweeDHiyU: Downloading webpage\n",
            "[youtube] qYNweeDHiyU: Downloading tv client config\n",
            "[youtube] qYNweeDHiyU: Downloading player f6e09c70\n",
            "[youtube] qYNweeDHiyU: Downloading tv player API JSON\n",
            "[youtube] qYNweeDHiyU: Downloading ios player API JSON\n",
            "[youtube] qYNweeDHiyU: Downloading m3u8 information\n",
            "[info] qYNweeDHiyU: Downloading 1 format(s): 251\n",
            "[download] audio1.wav has already been downloaded\n",
            "[ExtractAudio] Destination: audio1.wav\n",
            "Deleting original file audio1.orig.wav (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp\n",
        "!yt-dlp --extract-audio --audio-format wav -o \"audio1.wav\" \"https://www.youtube.com/watch?v=qYNweeDHiyU\"\n",
        "#!yt-dlp --extract-audio --audio-format wav -o \"audio1.wav\" \"https://www.youtube.com/watch?v=alfdI7S6wCY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"/content/audio1.wav\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"✅ File exists:\", file_path)\n",
        "else:\n",
        "    print(\"❌ File not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM7fWJ92ovrx",
        "outputId": "19428e8a-92ee-49b2-ce55-3fac716b1067"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File exists: /content/audio1.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px08t3tseHZ2"
      },
      "source": [
        "Installation of libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWUBL1BjeI3y",
        "outputId": "0557c34a-88fd-4319-f942-faa931522eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-sz5jjo0d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-sz5jjo0d\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.9.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.6.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.4.2)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->pke==2.0.0) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->pke==2.0.0) (2024.11.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet flashtext==2.7\n",
        "!pip install git+https://github.com/boudinfl/pke.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K2qN7MwMeiCJ"
      },
      "outputs": [],
      "source": [
        "# !pip install --quiet transformers==4.8.1\n",
        "# !pip install --quiet sentencepiece==0.1.95\n",
        "!pip install --quiet textwrap3==0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VfpT18_ie1MX"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet strsim==0.0.3\n",
        "!pip install --quiet sense2vec==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3A5P1T2e5Nc",
        "outputId": "0b7cf96b-8830-4d36-c88b-bc67e1e3ad7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 381 µs (started: 2025-03-10 12:21:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9sY9Ae2e9B6",
        "outputId": "f8d3dfc6-97a0-4e8c-e50f-a3fd25a047a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.37 ms (started: 2025-03-10 12:21:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDasQjV6Xee"
      },
      "source": [
        "### **MCQ Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "933BZpOSg0rB"
      },
      "source": [
        "Reading transcript"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"/content/audio1.wav\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"✅ File exists:\", file_path)\n",
        "else:\n",
        "    print(\"❌ File not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIRLIJcJo1dv",
        "outputId": "f257df47-be48-4bb0-b96a-86be8ce251e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File exists: /content/audio1.wav\n",
            "time: 1.09 ms (started: 2025-03-10 12:21:39 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wave\n",
        "\n",
        "file_path = \"/content/audio1.wav\"\n",
        "\n",
        "with wave.open(file_path, 'rb') as f:\n",
        "    print(\"Number of channels:\", f.getnchannels())\n",
        "    print(\"Sample width:\", f.getsampwidth())\n",
        "    print(\"Frame rate (sample rate):\", f.getframerate())\n",
        "    print(\"Number of frames:\", f.getnframes())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN0xIwM8pNn5",
        "outputId": "8faac68c-f94d-4fb1-ff53-b0bc63173799"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of channels: 2\n",
            "Sample width: 2\n",
            "Frame rate (sample rate): 48000\n",
            "Number of frames: 28821351\n",
            "time: 7.23 ms (started: 2025-03-10 12:21:39 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93qyUJTLgOlz",
        "outputId": "384ef805-3643-4bca-fbaa-e1ab342b0b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to r2u.stat.illinois\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [3 InRelease 14.2 kB/129 kB 11%] [Connected to r2u.sta\u001b[0m\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 3s (94.7 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "33 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Everybody's talking about artificial intelligence these days, AI. Machine learning is another hot topic. Are they the same thing or are they different? And if so, what are those differences? And deep learning is another one that comes into play. I actually did a video on these three, artificial intelligence, machine learning and deep learning, and talked about where they fit. And there were a lot of comments on that, and I read those comments, and I'd like to address some of the most frequently asked questions so that we can clear up some of the myths and misconceptions around this. In addition, something else has happened since that video was recorded, and that is the absolute explosion of this area of generative AI. Things like large language models and chatbots have seemed to be taking over the world. We see them everywhere. Really interesting technology. And then also things like deep fakes. These are all within the realm of AI, but how do they fit within each other? How are they related to each other? We're going to take a look at that in this video and try to explain how all these technologies relate and how we can use them. First off, a little bit of a disclaimer. I'm going to have to simplify some of these concepts in order to not make this video last for a week. So those of you that are really deep experts in the field, apologies in advance. But we're going to try to make this simple and that will involve some generalizations. First of all, to start with AI. Artificial intelligence is basically trying to simulate with a computer something that would match or exceed human intelligence. What is intelligence? Well, it could be a lot of different things, but generally we tend to think of it as the ability to learn and for and to reason. Things like that. That's what we're trying to do in the broad field of AI, of artificial intelligence. If we look at a timeline of AI, it really started back around this timeframe. In those days, it was very premature. Most people had not even heard of it. It basically was a research project. I can tell you, as an undergrad, which for me was back during these times, we were doing AI work. In fact, we would use programming languages like Lisp or ProLog. These kinds of things were kind of the predecessors to what became later expert systems. This was a technology. Again, some of these things existed previous, but that's when it really hit a kind of a critical mass and became more popularized. Expert systems of the 1980s, maybe in the 90s. Again, we used technologies like this. All of this was something that we did before we ever touched in to the next topic I'm going to talk about. That's the area of machine learning. Machine learning is, as its name implies, the machine is learning. I don't have to program it. I give it lots of information and it observes things. For instance, if I start doing this, if I give you this and then ask you to predict what's the next thing that's going to be there, well, you might get it, you might not. You have very limited training data to base this on. If I gave you one of those and then ask you what to predict what to happen next, well, you're probably going to say this and then you're going to say it's this and then you think you got it all figured out and then you see one of these and then all of a sudden I give you one of those and throw you a curveball. This, in fact, and then maybe it goes on like this. A machine learning algorithm is really good at looking at patterns and discovering patterns within data. The more training data you can give it, the more confident it can be in predicting. Predictions are one of the things that machine learning is particularly good at. Another thing is spotting outliers like this and saying, oh, that doesn't belong in, it looks different than all the other stuff because the sequence was broken. That's particularly useful in cybersecurity, the area that I work in, because we're looking for outliers. We're looking for users who are using the system in ways that they shouldn't be or ways that they don't typically do. This technology machine learning is particularly useful for us. Machine learning really came along and became more popularized in this time frame in the 2010s. Again, back when I was an undergrad writing my dinosaur to class, we were doing this kind of stuff. We never once talked about machine learning. It might have existed, but it really wasn't, hadn't hit the popular mindset yet. But this technology has matured greatly over the last few decades and now it becomes the basis of a lot we do going forward. The next layer of our Venn diagram involves deep learning. Well, it's deep learning in the sense that with deep learning, we use these things called neural networks. Neural networks are ways that in a computer we simulate and mimic the way the human brain works, at least to the extent that we understand how the brain works. And it's called deep because we have multiple layers of those neural networks. And the interesting thing about these is they will simulate the way a brain operates, but I don't know if you've noticed, but human brains can be a little bit unpredictable. You put certain things in, you don't always get the very same thing out and deep learning is the same way. In some cases, we're not actually able to fully understand why we get the results we do because there are so many layers to the neural network. It's a little bit hard to decompose and figure out exactly what's in there. But this has become a very important part and a very important advancement. That also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI. The most recent advancements in the field of artificial intelligence all really are in this space, the area of generative AI. Now I'm going to introduce a term that you may not be familiar with. It's the idea of foundation models. Foundation models is where we get some of these kinds of things. For instance, an example of a foundation model would be a large language model, which is where we take language and we model it. And we make predictions in this technology where if I see certain types of words, then I can sort of predict what the next set of words would be. I'm going to oversimplify here for the sake of simplicity. But think about this as a little bit like the autocomplete. When you start typing something in and then it predicts what your next word will be, except in this case with large language models, they're not predicting the next word. They're predicting the next sentence, the next paragraph, the next entire document. So there's a really an amazing exponential leap in what these things are able to do. And we call all of these technologies generative because they are generating new content. Some people have actually made the argument that the generative AI isn't really generative that these technologies are really just regurgitating, existing information and putting it in different format. Well, let me give you an analogy. If you take music, for instance, then every note has already been invented. So in a sense, every song is just a recombination, some other permutation of all the notes that already exist already and just putting them in a different order. Well, we don't say new music doesn't exist. People are still composing and creating new songs from the existing information. I'm going to say gen AI is similar. It's an analogy, so there are be some imperfections in it, but you get the general idea. Something new content can be generated out of these and there are a lot of different forms that this can take. With other types of models, are audio models, video models and things like that. Well, in fact, these we can use to create deep fakes. And deep fakes are examples where we're able to take, for instance, a person's voice and recreate that and then have it seem like the person said things they never said. Well, it's really useful in entertainment situations, in parodies and things like that, or if someone's losing their voice, then you could capture their voice and then they'd be able to type and you'd be able to hear it in their voice. But there's also a lot of cases where this stuff could be abused. The chat bots, again, come from this space, the deep fakes come from this space. But they're all part of generative AI and all part of these foundation models. And this, again, is the area that has really caused all of us to really pay attention to AI. The possibilities of generating new content, or in some cases summarizing existing content and giving us something that is bite size and manageable. This is what has gotten all of the attention. This is where the chat bots and all of these things come in. In the early days, AI's adoption started off pretty slowly. Most people didn't even know it existed and if they did, it was something that always seemed like it was about five to ten years away. But then machine learning, deep learning, and things like that came along and we started seeing some uptick. Then foundation models, Gen AI, and the light came along and this stuff went straight to the moon. These foundation models are what have changed the adoption curve. And now you see AI being adopted everywhere. And the thing for us to understand is where this is, where it fits in, and make sure that we can reap the benefits from all of this technology. If you like this video and want to see more like it, please like and subscribe. If you have any questions or want to share your thoughts about this topic, please leave a comment below.\n",
            "time: 3min 21s (started: 2025-03-10 12:21:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q openai-whisper\n",
        "!sudo apt update && sudo apt install -y ffmpeg  # Ensure FFmpeg is installed\n",
        "\n",
        "import whisper\n",
        "\n",
        "# Load model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe\n",
        "file_path = \"/content/audio1.wav\"\n",
        "result = model.transcribe(file_path)\n",
        "print(result[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#file_path = '/content/audio1.wav'\n",
        "\n",
        "#with open(file_path, 'r') as f:\n",
        " #   text = f.read()\n",
        "\n",
        "#print(text)\n",
        "import whisper\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"small\")   # Try \"tiny\", \"base\", \"medium\", or \"large\" based on need\n",
        "\n",
        "# Transcribe audio\n",
        "file_path = \"/content/audio1.wav\"\n",
        "result = model.transcribe(file_path)\n",
        "\n",
        "# Print the transcribed text\n",
        "print(result[\"text\"])  # ✅ This will print the actual transcription\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58I_v0QkqmDG",
        "outputId": "8049074e-b1f7-4460-8d26-81b331bbf170"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Everybody's talking about artificial intelligence these days, AI. Machine learning is another hot topic. Are they the same thing or are they different? And if so, what are those differences? And deep learning is another one that comes into play. I actually did a video on these three, artificial intelligence, machine learning, and deep learning, and talked about where they fit. And there were a lot of comments on that, and I read those comments, and I'd like to address some of the most frequently asked questions so that we can clear up some of the myths and misconceptions around this. In addition, something else has happened since that video was recorded, and that is the absolute explosion of this area of generative AI. Things like large language models and chatbots have seemed to be taking over the world. We see them everywhere. Really interesting technology. And then also things like deepfakes. These are all within the realm of AI, but how do they fit within each other? How are they related to each other? We're going to take a look at that in this video and try to explain how all these technologies relate and how we can use them. First off, a little bit of a disclaimer. I'm going to have to simplify some of these concepts in order to not make this video last for a week. So those of you that are really deep experts in the field, apologies in advance, but we're going to try to make this simple and that will involve some generalizations. First of all, let's start with AI. Artificial intelligence is basically trying to simulate with a computer something that would match or exceed human intelligence. What is intelligence? Well, it could be a lot of different things, but generally we tend to think of it as the ability to learn, to infer, and to reason, things like that. So that's what we're trying to do in the broad field of AI, of artificial intelligence. And if we look at a timeline of AI, it really kind of started back around this timeframe and in those days it was very premature. Most people had not even heard of it and it basically was a research project. But I can tell you as an undergrad, which for me was back during these times, we were doing AI work. In fact, we would use programming languages like Lisp or Prologue. And these kinds of things were kind of the predecessors to what became later expert systems. And this was a technology, again, some of these things existed previous, but that's when it really hit the kind of a critical mass and became more popularized. So expert systems of the 1980s, maybe in the 90s. And again, we use technologies like this. All of this was something that we did before we ever touched in to the next topic I'm going to talk about. And that's the area of machine learning. Machine learning is as its name implies. The machine is learning. I don't have to program it. I give it lots of information and it observes things. So for instance, if I start doing this, if I give you this and then ask you to predict what's the next thing that's going to be there, well, you might get it, you might not. You have very limited training data to base this on. But if I gave you one of those and then ask you what to predict, what would happen next, well, you're probably going to say this. And then you're going to say it's this. And then you think you got it all figured out. And then you see one of these. And then all of a sudden, I give you one of those and throw you a curveball. So this, in fact, and then maybe it goes on like this. So a machine learning algorithm is really good at looking at patterns and discovering patterns within data. The more training data you can give it, the more confident it can be in predicting. So predictions are one of the things that machine learning is particularly good at. Another thing is spotting outliers like this and saying, oh, that doesn't belong in. It looks different than all the other stuff because the sequence was broken. So that's particularly useful in cybersecurity, the area that I work in, because we're looking for outliers. We're looking for users who are using the system in ways that they shouldn't be or ways that they don't typically do. So this technology, machine learning, is particularly useful for us. And machine learning really came along and became more popularized in this time frame, in the 2010s. And again, back when I was an undergrad riding my dinosaur to class, we were doing this kind of stuff. We never once talked about machine learning. It might have existed, but it really wasn't, hadn't hit the popular mindset yet. But this technology has matured greatly over the last few decades, and now it becomes the basis of a lot we do going forward. The next layer of our Venn diagram involves deep learning. Well, it's deep learning in the sense that with deep learning, we use these things called neural networks. Neural networks are ways that in a computer, we simulate and mimic the way the human brain works, at least to the extent that we understand how the brain works. And it's called deep because we have multiple layers of those neural networks. And the interesting thing about these is they will simulate the way a brain operates, but I don't know if you've noticed, but human brains can be a little bit unpredictable. You put certain things in, you don't always get the very same thing out, and deep learning is the same way. In some cases, we're not actually able to fully understand why we get the results we do because there are so many layers to the neural network. That's a little bit hard to decompose and figure out exactly what's in there. But this has become a very important part and a very important advancement. That also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI. The most recent advancements in the field of artificial intelligence all really are in this space, the area of generative AI. Now, I'm going to introduce a term that you may not be familiar with. It's the idea of foundation models. Foundation models is where we get some of these kinds of things. For instance, an example of a foundation model would be a large language model, which is where we take language and we model it. And we make predictions in this technology where if I see certain types of words, then I can sort of predict what the next set of words will be. I'm going to oversimplify here for the sake of simplicity, but think about this as a little bit like the autocomplete. When you start typing something in and then it predicts what your next word will be, except in this case with large language models, they're not predicting the next word. They're predicting the next sentence, the next paragraph, the next entire document. So there's a really an amazing exponential leap in what these things are able to do. And we call all of these technologies generative because they are generating new content. Some people have actually made the argument that the generative AI isn't really generative, that these technologies are really just regurgitating existing information and putting it in different format. Well, let me give you an analogy. If you take music, for instance, then every note has already been invented. So in a sense, every song is just a recombination, some other permutation of all the notes that already exist already and just putting them in a different order. Well, we don't say new music doesn't exist. People are still composing and creating new songs from the existing information. I'm going to say Gen AI is similar. It's an analogy, so there'll be some imperfections in it, but you get the general idea. Really new content can be generated out of these. And there are a lot of different forms that this can take. With other types of models are audio models, video models, and things like that. Well, in fact, these we can use to create deepfakes. And deepfakes are examples where we're able to take, for instance, a person's voice and recreate that and then have it seem like the person said things they never said. Well, it's really useful in entertainment situations, in parodies and things like that, or if someone's losing their voice, then you could capture their voice and then they'd be able to type and you'd be able to hear it in their voice. But there's also a lot of cases where this stuff could be abused. The chatbots, again, come from this space. The deepfakes come from this space, but they're all part of generative AI and all part of these foundation models. And this, again, is the area that has really caused all of us to really pay attention to AI. The possibilities of generating new content, or in some cases summarizing existing content and giving us something that is bite-size and manageable. This is what has gotten all of the attention. This is where the chatbots and all of these things come in. In the early days, AI's adoption started off pretty slowly. Most people didn't even know it existed. And if they did, it was something that always seemed like it was about five to 10 years away. But then machine learning, deep learning, and things like that came along and we started seeing some uptick. Then foundation models, Gen AI and the light came along and this stuff went straight to the moon. These foundation models are what have changed the adoption curve. And now you see AI being adopted everywhere. And the thing for us to understand is where this is, where it fits in, and make sure that we can reap the benefits from all of this technology. If you like this video and want to see more like it, please like and subscribe. If you have any questions or want to share your thoughts about this topic, please leave a comment below.\n",
            "time: 8min 24s (started: 2025-03-10 12:25:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSae61whiRmh",
        "outputId": "a93bf718-b551-4bfa-8064-fd4dc8d2bd6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Everybody's talking about artificial intelligence these days, AI. Machine learning is another hot topic. Are they the same thing or are they\n",
            "different? And if so, what are those differences? And deep learning is another one that comes into play. I actually did a video on these three,\n",
            "artificial intelligence, machine learning, and deep learning, and talked about where they fit. And there were a lot of comments on that, and I read\n",
            "those comments, and I'd like to address some of the most frequently asked questions so that we can clear up some of the myths and misconceptions\n",
            "around this. In addition, something else has happened since that video was recorded, and that is the absolute explosion of this area of generative AI.\n",
            "Things like large language models and chatbots have seemed to be taking over the world. We see them everywhere. Really interesting technology. And\n",
            "then also things like deepfakes. These are all within the realm of AI, but how do they fit within each other? How are they related to each other?\n",
            "We're going to take a look at that in this video and try to explain how all these technologies relate and how we can use them. First off, a little bit\n",
            "of a disclaimer. I'm going to have to simplify some of these concepts in order to not make this video last for a week. So those of you that are really\n",
            "deep experts in the field, apologies in advance, but we're going to try to make this simple and that will involve some generalizations. First of all,\n",
            "let's start with AI. Artificial intelligence is basically trying to simulate with a computer something that would match or exceed human intelligence.\n",
            "What is intelligence? Well, it could be a lot of different things, but generally we tend to think of it as the ability to learn, to infer, and to\n",
            "reason, things like that. So that's what we're trying to do in the broad field of AI, of artificial intelligence. And if we look at a timeline of AI,\n",
            "it really kind of started back around this timeframe and in those days it was very premature. Most people had not even heard of it and it basically\n",
            "was a research project. But I can tell you as an undergrad, which for me was back during these times, we were doing AI work. In fact, we would use\n",
            "programming languages like Lisp or Prologue. And these kinds of things were kind of the predecessors to what became later expert systems. And this was\n",
            "a technology, again, some of these things existed previous, but that's when it really hit the kind of a critical mass and became more popularized. So\n",
            "expert systems of the 1980s, maybe in the 90s. And again, we use technologies like this. All of this was something that we did before we ever touched\n",
            "in to the next topic I'm going to talk about. And that's the area of machine learning. Machine learning is as its name implies. The machine is\n",
            "learning. I don't have to program it. I give it lots of information and it observes things. So for instance, if I start doing this, if I give you this\n",
            "and then ask you to predict what's the next thing that's going to be there, well, you might get it, you might not. You have very limited training data\n",
            "to base this on. But if I gave you one of those and then ask you what to predict, what would happen next, well, you're probably going to say this. And\n",
            "then you're going to say it's this. And then you think you got it all figured out. And then you see one of these. And then all of a sudden, I give you\n",
            "one of those and throw you a curveball. So this, in fact, and then maybe it goes on like this. So a machine learning algorithm is really good at\n",
            "looking at patterns and discovering patterns within data. The more training data you can give it, the more confident it can be in predicting. So\n",
            "predictions are one of the things that machine learning is particularly good at. Another thing is spotting outliers like this and saying, oh, that\n",
            "doesn't belong in. It looks different than all the other stuff because the sequence was broken. So that's particularly useful in cybersecurity, the\n",
            "area that I work in, because we're looking for outliers. We're looking for users who are using the system in ways that they shouldn't be or ways that\n",
            "they don't typically do. So this technology, machine learning, is particularly useful for us. And machine learning really came along and became more\n",
            "popularized in this time frame, in the 2010s. And again, back when I was an undergrad riding my dinosaur to class, we were doing this kind of stuff.\n",
            "We never once talked about machine learning. It might have existed, but it really wasn't, hadn't hit the popular mindset yet. But this technology has\n",
            "matured greatly over the last few decades, and now it becomes the basis of a lot we do going forward. The next layer of our Venn diagram involves deep\n",
            "learning. Well, it's deep learning in the sense that with deep learning, we use these things called neural networks. Neural networks are ways that in\n",
            "a computer, we simulate and mimic the way the human brain works, at least to the extent that we understand how the brain works. And it's called deep\n",
            "because we have multiple layers of those neural networks. And the interesting thing about these is they will simulate the way a brain operates, but I\n",
            "don't know if you've noticed, but human brains can be a little bit unpredictable. You put certain things in, you don't always get the very same thing\n",
            "out, and deep learning is the same way. In some cases, we're not actually able to fully understand why we get the results we do because there are so\n",
            "many layers to the neural network. That's a little bit hard to decompose and figure out exactly what's in there. But this has become a very important\n",
            "part and a very important advancement. That also reached some popularity during the 2010s and as something that we use still today as the basis for\n",
            "our next area of AI. The most recent advancements in the field of artificial intelligence all really are in this space, the area of generative AI.\n",
            "Now, I'm going to introduce a term that you may not be familiar with. It's the idea of foundation models. Foundation models is where we get some of\n",
            "these kinds of things. For instance, an example of a foundation model would be a large language model, which is where we take language and we model\n",
            "it. And we make predictions in this technology where if I see certain types of words, then I can sort of predict what the next set of words will be.\n",
            "I'm going to oversimplify here for the sake of simplicity, but think about this as a little bit like the autocomplete. When you start typing something\n",
            "in and then it predicts what your next word will be, except in this case with large language models, they're not predicting the next word. They're\n",
            "predicting the next sentence, the next paragraph, the next entire document. So there's a really an amazing exponential leap in what these things are\n",
            "able to do. And we call all of these technologies generative because they are generating new content. Some people have actually made the argument that\n",
            "the generative AI isn't really generative, that these technologies are really just regurgitating existing information and putting it in different\n",
            "format. Well, let me give you an analogy. If you take music, for instance, then every note has already been invented. So in a sense, every song is\n",
            "just a recombination, some other permutation of all the notes that already exist already and just putting them in a different order. Well, we don't\n",
            "say new music doesn't exist. People are still composing and creating new songs from the existing information. I'm going to say Gen AI is similar. It's\n",
            "an analogy, so there'll be some imperfections in it, but you get the general idea. Really new content can be generated out of these. And there are a\n",
            "lot of different forms that this can take. With other types of models are audio models, video models, and things like that. Well, in fact, these we\n",
            "can use to create deepfakes. And deepfakes are examples where we're able to take, for instance, a person's voice and recreate that and then have it\n",
            "seem like the person said things they never said. Well, it's really useful in entertainment situations, in parodies and things like that, or if\n",
            "someone's losing their voice, then you could capture their voice and then they'd be able to type and you'd be able to hear it in their voice. But\n",
            "there's also a lot of cases where this stuff could be abused. The chatbots, again, come from this space. The deepfakes come from this space, but\n",
            "they're all part of generative AI and all part of these foundation models. And this, again, is the area that has really caused all of us to really pay\n",
            "attention to AI. The possibilities of generating new content, or in some cases summarizing existing content and giving us something that is bite-size\n",
            "and manageable. This is what has gotten all of the attention. This is where the chatbots and all of these things come in. In the early days, AI's\n",
            "adoption started off pretty slowly. Most people didn't even know it existed. And if they did, it was something that always seemed like it was about\n",
            "five to 10 years away. But then machine learning, deep learning, and things like that came along and we started seeing some uptick. Then foundation\n",
            "models, Gen AI and the light came along and this stuff went straight to the moon. These foundation models are what have changed the adoption curve.\n",
            "And now you see AI being adopted everywhere. And the thing for us to understand is where this is, where it fits in, and make sure that we can reap the\n",
            "benefits from all of this technology. If you like this video and want to see more like it, please like and subscribe. If you have any questions or\n",
            "want to share your thoughts about this topic, please leave a comment below.\n",
            "\n",
            "\n",
            "time: 8min 32s (started: 2025-03-10 12:33:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "from textwrap3 import wrap  # Ensure you have textwrap3 installed: pip install textwrap3\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"small\")\n",
        "\n",
        "# Transcribe audio\n",
        "file_path = \"/content/audio1.wav\"\n",
        "result = model.transcribe(file_path)\n",
        "\n",
        "# Store the transcribed text\n",
        "text = result[\"text\"]  # ✅ Extracting the text\n",
        "\n",
        "# Wrap the text at 150 characters per line\n",
        "for wrp in wrap(text, 150):\n",
        "    print(wrp)\n",
        "\n",
        "print(\"\\n\")  # Just for spacing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gJDctl0ismM"
      },
      "source": [
        "# **Text Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttTe5CyIjJay",
        "outputId": "6d3238b8-ecb0-4465-df45-c0e1a9ab5da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.3 s (started: 2025-03-10 12:41:58 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "if summary_model is None:\n",
        "    raise ValueError(\"Model failed to load! Check if 't5-base' is available.\")\n",
        "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "summary_model = summary_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2O5TAFBjLiO",
        "outputId": "16023a9f-c644-4841-cf37-6076912f0ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.77 ms (started: 2025-03-10 12:42:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rVc1FfrM1Rm",
        "outputId": "fb1c546e-1bcc-4240-894b-a78b8c4b88ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 499 ms (started: 2025-03-10 12:42:11 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Manually delete the 'punkt' tokenizer if it exists\n",
        "nltk_data_path = os.path.join(nltk.data.find(\"tokenizers\"), \"punkt\")\n",
        "if os.path.exists(nltk_data_path):\n",
        "    shutil.rmtree(nltk_data_path)\n",
        "\n",
        "print(\"Deleted existing 'punkt' package (if it existed).\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgfjjvqyzqXq",
        "outputId": "9e631f72-011a-42d2-ef7b-c5ed61fd5fae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted existing 'punkt' package (if it existed).\n",
            "time: 5.85 ms (started: 2025-03-10 12:42:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "rwXjKZWqjOFt",
        "outputId": "f3b26126-54ba-4434-d757-461cead6b81f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-018b91f6bce4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0msummarized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-018b91f6bce4>\u001b[0m in \u001b[0;36msummarizer\u001b[0;34m(text, model, tokenizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostprocesstext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-018b91f6bce4>\u001b[0m in \u001b[0;36mpostprocesstext\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpostprocesstext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 29.3 s (started: 2025-03-10 12:42:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def postprocesstext (content):\n",
        "  final=\"\"\n",
        "  for sent in sent_tokenize(content):\n",
        "    sent = sent.capitalize()\n",
        "    final = final +\" \"+sent\n",
        "  return final\n",
        "\n",
        "\n",
        "def summarizer(text,model,tokenizer):\n",
        "  text = text.strip().replace(\"\\n\",\" \")\n",
        "  text = \"summarize: \"+text\n",
        "  # print (text)\n",
        "  max_len = 512\n",
        "  encoding = tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=3,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  min_length = 75,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  summary = dec[0]\n",
        "  summary = postprocesstext(summary)\n",
        "  summary= summary.strip()\n",
        "\n",
        "  return summary\n",
        "\n",
        "\n",
        "summarized_text = summarizer(text,summary_model,summary_tokenizer)\n",
        "\n",
        "\n",
        "print (\"\\noriginal Text >>\")\n",
        "for wrp in wrap(text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n",
        "print (\"Summarized Text >>\")\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Summarization using Hugging Face Transformers**"
      ],
      "metadata": {
        "id": "KCEi_uzLfFkC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCf3NFuLViOi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_text=summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "print(summarized_text)"
      ],
      "metadata": {
        "id": "4iX8WMkWfPf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6gmVQaPkYy2"
      },
      "source": [
        "# **Keyword Extraction**\n",
        "(Noun phrases and keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gglvdE_9krZB"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pke\n",
        "import traceback\n",
        "\n",
        "def get_nouns_multipartite(content):\n",
        "    out=[]\n",
        "    try:\n",
        "        extractor = pke.unsupervised.MultipartiteRank()\n",
        "        extractor.load_document(input=content,language='en')\n",
        "        #    not contain punctuation marks or stopwords as candidates.\n",
        "        pos = {'PROPN','NOUN'}\n",
        "\n",
        "        stoplist = list(string.punctuation)\n",
        "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "        stoplist += stopwords.words('english')\n",
        "        # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
        "        extractor.candidate_selection(pos=pos)\n",
        "\n",
        "        extractor.candidate_weighting(alpha=1.1,\n",
        "                                      threshold=0.75,\n",
        "                                      method='average')\n",
        "        keyphrases = extractor.get_n_best(n=15)\n",
        "\n",
        "\n",
        "        for val in keyphrases:\n",
        "            out.append(val[0])\n",
        "    except:\n",
        "        out = []\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpeYs8j1kiu7"
      },
      "outputs": [],
      "source": [
        "from flashtext import KeywordProcessor\n",
        "\n",
        "\n",
        "def get_keywords(originaltext,summarytext):\n",
        "  keywords = get_nouns_multipartite(originaltext)\n",
        "  print (\"keywords unsummarized: \",keywords)\n",
        "  keyword_processor = KeywordProcessor()\n",
        "  for keyword in keywords:\n",
        "    keyword_processor.add_keyword(keyword)\n",
        "\n",
        "  keywords_found = keyword_processor.extract_keywords(summarytext)\n",
        "  keywords_found = list(set(keywords_found))\n",
        "  print (\"keywords_found in summarized: \",keywords_found)\n",
        "\n",
        "  important_keywords =[]\n",
        "  for keyword in keywords:\n",
        "    if keyword in keywords_found:\n",
        "      important_keywords.append(keyword)\n",
        "\n",
        "  return important_keywords[:4]\n",
        "\n",
        "\n",
        "imp_keywords = get_keywords(text,summarized_text)\n",
        "print (imp_keywords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KFdZaCFlY_L"
      },
      "source": [
        "# **Question Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQSSyb-1lhnv"
      },
      "outputs": [],
      "source": [
        "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_model = question_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJSyiyGpluYA"
      },
      "outputs": [],
      "source": [
        "def get_question(context,answer,model,tokenizer):\n",
        "  text = \"context: {} answer: {}\".format(context,answer)\n",
        "  encoding = tokenizer.encode_plus(text,max_length=384, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=72)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "\n",
        "\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n",
        "\n",
        "ques_ans={}\n",
        "for answer in imp_keywords:\n",
        "  ques = get_question(summarized_text,answer,question_model,question_tokenizer)\n",
        "  print (ques)\n",
        "\n",
        "  key = ques\n",
        "  value = answer\n",
        "  ques_ans[key] = value\n",
        "\n",
        "  print (answer.capitalize())\n",
        "  print (\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUmFaodHzqz-"
      },
      "outputs": [],
      "source": [
        "print(ques_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYHEAF1OmNC5"
      },
      "source": [
        "# **Generate Distractor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMHrgacNmTtj"
      },
      "source": [
        "Filter keywords with Maximum marginal Relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBsVUdvHBiQC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sense2vec import Sense2Vec\n",
        "s2v = Sense2Vec().from_disk('s2v_old')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDHiwX3QBkQi"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "# paraphrase-distilroberta-base-v1\n",
        "sentence_transformer_model = SentenceTransformer('msmarco-distilbert-base-v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raFR0ET1CT9-"
      },
      "outputs": [],
      "source": [
        "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
        "normalized_levenshtein = NormalizedLevenshtein()\n",
        "\n",
        "def filter_same_sense_words(original,wordlist):\n",
        "  filtered_words=[]\n",
        "  base_sense =original.split('|')[1]\n",
        "  print (base_sense)\n",
        "  for eachword in wordlist:\n",
        "    if eachword[0].split('|')[1] == base_sense:\n",
        "      filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
        "  return filtered_words\n",
        "\n",
        "def get_highest_similarity_score(wordlist,wrd):\n",
        "  score=[]\n",
        "  for each in wordlist:\n",
        "    score.append(normalized_levenshtein.similarity(each.lower(),wrd.lower()))\n",
        "  return max(score)\n",
        "\n",
        "def sense2vec_get_words(word,s2v,topn,question):\n",
        "    output = []\n",
        "    print (\"word \",word)\n",
        "    try:\n",
        "      sense = s2v.get_best_sense(word, senses= [\"NOUN\", \"PERSON\",\"PRODUCT\",\"LOC\",\"ORG\",\"EVENT\",\"NORP\",\"WORK OF ART\",\"FAC\",\"GPE\",\"NUM\",\"FACILITY\"])\n",
        "      most_similar = s2v.most_similar(sense, n=topn)\n",
        "      # print (most_similar)\n",
        "      output = filter_same_sense_words(sense,most_similar)\n",
        "      print (\"Similar \",output)\n",
        "    except:\n",
        "      output =[]\n",
        "\n",
        "    threshold = 0.6\n",
        "    final=[word]\n",
        "    checklist =question.split()\n",
        "    for x in output:\n",
        "      if get_highest_similarity_score(final,x)<threshold and x not in final and x not in checklist:\n",
        "        final.append(x)\n",
        "\n",
        "    return final[1:]\n",
        "\n",
        "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
        "\n",
        "    # Extract similarity within words, and between words and the document\n",
        "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
        "    word_similarity = cosine_similarity(word_embeddings)\n",
        "\n",
        "    # Initialize candidates and already choose best keyword/keyphrase\n",
        "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "\n",
        "    for _ in range(top_n - 1):\n",
        "        # Extract similarities within candidates and\n",
        "        # between candidates and selected keywords/phrases\n",
        "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "        # Calculate MMR\n",
        "        mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
        "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "        # Update keywords & candidates\n",
        "        keywords_idx.append(mmr_idx)\n",
        "        candidates_idx.remove(mmr_idx)\n",
        "\n",
        "    return [words[idx] for idx in keywords_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IodbjyzCXnq"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_distractors_wordnet(word):\n",
        "    distractors=[]\n",
        "    try:\n",
        "      syn = wn.synsets(word,'n')[0]\n",
        "\n",
        "      word= word.lower()\n",
        "      orig_word = word\n",
        "      if len(word.split())>0:\n",
        "          word = word.replace(\" \",\"_\")\n",
        "      hypernym = syn.hypernyms()\n",
        "      if len(hypernym) == 0:\n",
        "          return distractors\n",
        "      for item in hypernym[0].hyponyms():\n",
        "          name = item.lemmas()[0].name()\n",
        "          #print (\"name \",name, \" word\",orig_word)\n",
        "          if name == orig_word:\n",
        "              continue\n",
        "          name = name.replace(\"_\",\" \")\n",
        "          name = \" \".join(w.capitalize() for w in name.split())\n",
        "          if name is not None and name not in distractors:\n",
        "              distractors.append(name)\n",
        "    except:\n",
        "      print (\"Wordnet distractors not found\")\n",
        "    return distractors\n",
        "\n",
        "def get_distractors (word,origsentence,sense2vecmodel,sentencemodel,top_n,lambdaval):\n",
        "  distractors = sense2vec_get_words(word,sense2vecmodel,top_n,origsentence)\n",
        "  print (\"distractors \",distractors)\n",
        "  if len(distractors) ==0:\n",
        "    return distractors\n",
        "  distractors_new = [word.capitalize()]\n",
        "  distractors_new.extend(distractors)\n",
        "  # print (\"distractors_new .. \",distractors_new)\n",
        "\n",
        "  embedding_sentence = origsentence+ \" \"+word.capitalize()\n",
        "  # embedding_sentence = word\n",
        "  keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
        "  distractor_embeddings = sentencemodel.encode(distractors_new)\n",
        "\n",
        "  # filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors,4,0.7)\n",
        "  max_keywords = min(len(distractors_new),5)\n",
        "  filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors_new,max_keywords,lambdaval)\n",
        "  # filtered_keywords = filtered_keywords[1:]\n",
        "  final = [word.capitalize()]\n",
        "  for wrd in filtered_keywords:\n",
        "    if wrd.lower() !=word.lower():\n",
        "      final.append(wrd.capitalize())\n",
        "  final = final[1:]\n",
        "  return final\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG0k9lFS4Prs"
      },
      "outputs": [],
      "source": [
        "for key, value in ques_ans.items():\n",
        "    keyword=value\n",
        "    sent=key\n",
        "    print(sent)\n",
        "    print(keyword)\n",
        "    print (get_distractors(keyword,sent,s2v,sentence_transformer_model,40,0.2))\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}